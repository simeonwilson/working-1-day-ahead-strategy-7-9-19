{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn.metrics import accuracy_score  \n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statistics import mean \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import copy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\t\n",
    "from sklearn.utils import resample\n",
    "seed = 1234\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def data_maker(data):\n",
    "    \n",
    "    \n",
    "    data[\"created_at1\"] = pd.to_datetime(data['created_at'])\n",
    "    data[\"created_at\"] = data[\"created_at1\"].dt.strftime('%m/%d/%Y')\n",
    "    days =  (data.created_at.unique())\n",
    "    datan = pd.DataFrame()\n",
    "        \n",
    "    datan[\"dates\"] = days\n",
    "    #datan.to_csv('C:/Users/simeon/Documents/data_test.csv')    \n",
    "    \n",
    "    data = data.set_index(\"created_at\")\n",
    "    #data.to_csv('C:/Users/simeon/Documents/data_test.csv')   \n",
    "    tot_tweet =[]\n",
    "    tot_bull = []\n",
    "    tot_bear =[]\n",
    "    grouped_daily_tweets = []\n",
    "    for i in range(len(days)):\n",
    "            \n",
    "        d_use = datan[\"dates\"][i]\n",
    "        #print(d_use)\n",
    "        value=str(d_use)\n",
    "        sub = ((data.loc[[value]]))\n",
    "        sub[\"body\"]\n",
    "                \n",
    "        L = len(sub)\n",
    "        tot_tweet.append(L)\n",
    "        sub1 = sub[(sub.sentiment == {'class': 'bullish', 'name': 'Bullish'}) | (sub.sentiment ==  'Bullish')]\n",
    "        BL = len(sub1)\n",
    "        sub2 = sub[(sub.sentiment == {'class': 'bearish', 'name': 'Bearish'}) | (sub.sentiment == 'Bearish')]\n",
    "        BL2 = len(sub2)\n",
    "        tot_bull.append(BL)\n",
    "        tot_bear.append(BL2)\n",
    "        grouped = sub.groupby([\"created_at\"])[\"body\"].apply(list)\n",
    "        grouped_daily_tweets.append(grouped[0])\n",
    "            \n",
    "    datan[\"tot_bull\"] = tot_bull\n",
    "    datan[\"tot_bear\"] = tot_bear\n",
    "    datan[\"tot_tweet\"] = tot_tweet\n",
    "    datan[\"tweets\"] = grouped_daily_tweets\n",
    "    datan[\"mistake\"] = 0 \n",
    "    datan.loc[datan[\"tot_tweet\"] < datan[\"tot_bear\"], \"mistake\"] = 1\n",
    "    datan.loc[datan[\"tot_tweet\"] < datan[\"tot_bull\"], \"mistake\"] = 1\n",
    "    datan.loc[datan[\"mistake\"] ==1 , \"tot_tweet\"] = 1 \n",
    "    datan.loc[datan[\"mistake\"] ==1 , \"tot_bull\"] = 0 \n",
    "    datan.loc[datan[\"mistake\"] ==1 , \"tot_bear\"] = 0 \n",
    "    datan[\"polarity\"] = ((datan[\"tot_bull\"] - datan[\"tot_bear\"])/datan[\"tot_tweet\"])\n",
    "    datan[\"roll_avg\"] = datan.polarity.rolling(window=3).mean()\n",
    "    change = [100 * (b - a) / a for a, b in zip(datan[\"tot_tweet\"][::1], datan[\"tot_tweet\"][1::1])]\n",
    "    change.append(0)\n",
    "    datan[\"percent_change\"] = change[:]\n",
    "    av =  datan.tot_tweet.rolling(window=10).mean()\n",
    "    datan[\"mv10\"] = (datan[\"tot_tweet\"]/av)\n",
    "    datan = datan.drop([\"mistake\"], axis = 1)\n",
    "    global new_data\n",
    "    new_data = datan[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def stop_remover(Input_data):\n",
    "    \n",
    "    Input_data = pd.DataFrame(Input_data)\n",
    "    Input_data['tweets'] = Input_data['tweets'].astype(str).str.replace('\\d+', '')\n",
    "    \n",
    "    Input_data['tweets'] = Input_data['tweets'].astype(str).apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    Input_data['tweets'] = Input_data['tweets'].astype(str).str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    Input_data['tweets_no_stop'] = Input_data['tweets'].astype(str).str.replace(pat, '')\n",
    "    Input_data['tweets_no_stop'] = Input_data['tweets_no_stop'].str.replace(r'\\s+', ' ')\n",
    "    \n",
    "    global new\n",
    "    new = Input_data[:]\n",
    "    \n",
    "    \n",
    "names = [\"PowerShares QQQ Trust, Series 1\",\"SPDR S&P 500 ETF Trust\",\"facebook\",\"Facebook\",\"american express\",\"American Express Company\",\"American Express\",\"Boeing\",\"Boeing Co\",\"Chevron Corporation\",\"Chevron\",\"coca cola\",\"Coca-Cola\",\"Coca Cola\",\"DuPont\",\"Exxon Mobile \",\"Exxon Mobile Corporation\",\"ExxonMobil\",\"Exxon\",\"General Electric\",'general electric',\"General Electri Company\",\"Goldman Sachs Group\",\"Goldman Sachs\",\"Goldman Sachs Group inc\",\"goldman\",\"Godlman\",\"home depot\",\"Home Depot\",\"IBM\",\"Intel\",\"Intel Corporation\",\"johnson & johnson\",\"Johnson & Johnson\",\"Johnson\",\"Johnson and Johnson\",\"JP Morgan\",\"JPMorgan\",\"JPMorgan Chase\",\"3M Co\",\"3M\",\"3m\",\"Merck\",\"Merck & Co.\", \"Microsoft Corporation\",\"microsoft\",\"Microsoft\" ,\"pfizer\",\"Pfizer\" , \"Proctor and Gamble\", \"proctor & gamble\",\"Procter & Gabmle\" , \"caterpillar\", \"Caterpillar\", \"CAT\", \"cisco\",\"CISCO\",\"Cisco\",\"Cisco Systems\",\"travelers companies\",\"Travelers Companies\",\"UnitedHealth Group\",\"United Technologies Corporation\",\"VISA\",\"visa\",\"Visa\",\"Verizon\",\"verizon\",\"Verizon Communications\",\"Walmrat\",\"walmart\",\"google\", \"Google\", \"Nike\", \"nike\", \"McDonald's\",\"Mcdonald's\", \"mcdonald's\", \"Disney\", \"disney\", \"Walt Disney\" ]\n",
    "\n",
    "\n",
    "def name_remover(Input_data):\n",
    "    \n",
    "    Input_data = pd.DataFrame(Input_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(names))\n",
    "    Input_data['bodyn'] = Input_data['body'].astype(str).str.replace(pat, '')\n",
    "    Input_data['body'] = Input_data['bodyn'].str.replace(r'\\s+', ' ')\n",
    "    global new\n",
    "    new = Input_data[:]\n",
    "    \n",
    "    \n",
    "tickers = ['$MCD','$DIS','$WMT','$VZ','$V','$UTX','$UNH','$TRV','$CSCO','$CAT','$PG','$PFE','$MSFT','$MRK','$MMM','$JPM','$JNJ','$INTC','$IBM','$HD','$GS','$GE',\"$XOM\",'$DD','$KO',\"$CVX\",\"$BA\",\"$AXP\",\"$FB\",\"$QQQ\",\"$SPY\",\"$NKE\"]\n",
    "\n",
    "def dup_ender(data, ticker = ''):\n",
    "    tot_tic = []\n",
    "    stock_tic = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        tot = sum(data[\"body\"][i].count(x) for x in (tickers))\n",
    "        \n",
    "        stock = data[\"body\"][i].count(ticker) \n",
    "        tot_tic.append(tot)\n",
    "        stock_tic.append(stock)\n",
    "    data[\"tot_tic\"] = tot_tic\n",
    "    data[\"stock_tic\"] = stock\n",
    "    #print( data[\"tot_tic\"])\n",
    "    #print(data[\"stock_tic\"])\n",
    "    data[\"r\"] = np.where(data['tot_tic']>data[\"stock_tic\"], 1, 0)\n",
    "    data[\"body\"] = np.where(data['r'] == 1,'',data[\"body\"])\n",
    "    #for i in range(len(data)):\n",
    "        #print(i)\n",
    "        \n",
    "       # if data[\"stock_tic\"][i] < data[\"tot_tic\"][i]:\n",
    "        #    data[\"body\"][i] = ['']\n",
    "    global new1\n",
    "    new1 = data[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def shebang(data, data_P, ticker1 = \"\",  ):\n",
    "    name_remover(data)\n",
    "    data = new[:]\n",
    "    dup_ender(data, ticker = ticker1)\n",
    "    data = new1[:]\n",
    "    data_maker(data)\n",
    "    datan = new_data[:]\n",
    "    data_P[\"1_day_return\"] = data_P.CLOSE.pct_change(1)\n",
    "    data_P['1_day_return'] = data_P['1_day_return'].shift(-1)\n",
    "    data_P[\"3_day_return\"] = data_P.CLOSE.pct_change(3)\n",
    "    data_P['3_day_return'] = data_P['3_day_return'].shift(-3)\n",
    "    data_P[\"5_day_return\"] = data_P.CLOSE.pct_change(5)\n",
    "    data_P['5_day_return'] = data_P['5_day_return'].shift(-5)\n",
    "    stop_remover(datan)\n",
    "    datan = new[:]\n",
    "    datan['dates'] = pd.to_datetime(datan['dates'], errors='coerce')\n",
    "    data_P['Date'] = pd.to_datetime(data_P['Date'], errors='coerce')\n",
    "    datan = datan.set_index('dates')\n",
    "    data_P = data_P.set_index('Date')\n",
    "    datan = datan.iloc[::-1]\n",
    "    new_d = pd.concat([data_P, datan],ignore_index=False, axis = 1 , join = \"outer\", join_axes = [data_P.index])\n",
    "    global F\n",
    "    F = new_d[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "#GS = pd.read_json(\"C:/Users/simeon/Documents/GS.txt\")\n",
    "#GS_P = pd.read_csv(\"C:/Users/simeon/Documents/GS.csv\")\n",
    "   \n",
    "#shebang(GS, GS_P, \"$GS\")  \n",
    "#GS = F[:]\n",
    "\n",
    "#GS.to_csv(r'C:/Users/simeon/Documents/GS_test.csv')\n",
    "\n",
    "#print(GS.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "#GS['1_day_return'].shift(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GS1 = pd.read_json(\"C:/Users/simeon/Documents/GS.txt\")\n",
    "#GS1.to_csv(r'C:/Users/simeon/Documents/clean/GS1.csv')\n",
    "\n",
    "\n",
    "\n",
    "GS2 = pd.read_csv(\"C:/Users/simeon/Documents/clean/GS_latest.csv\")\n",
    "# foldman sachs updated testing\n",
    "\n",
    "GS1['created_at']\n",
    "GS2[\"date\"].astype(str)\n",
    "\n",
    "print(GS2.dtypes)\n",
    "print(GS1.dtypes)\n",
    "\n",
    "\n",
    "#pip install python-dateutil\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "GS2['date'] = GS2[\"date\"].apply(lambda x:  parser.parse(x))\n",
    "\n",
    "\n",
    "GS = GS1[['created_at','sentiment','body']].copy()\n",
    "dates = GS2['date'].tolist() + GS1['created_at'].tolist()\n",
    "\n",
    "\n",
    "# Creating an empty Dataframe with column names only\n",
    "dfObj = pd.DataFrame(columns=['User_ID', 'UserName', 'Action'])\n",
    "print(\"Empty Dataframe \", dfObj, sep='\\n')\n",
    "\n",
    "# Creating an empty Dataframe with column names only\n",
    "GS0 = pd.DataFrame(columns=['created_at', 'sentiment', 'body'])\n",
    "\n",
    "GS0['created_at'] = dates\n",
    "\n",
    "sentiment = GS2['sentiment'].tolist() + GS1['sentiment'].tolist()\n",
    "\n",
    "GS0['sentiment'] = sentiment\n",
    "\n",
    "body = GS2['body'].tolist() + GS1['body'].tolist()\n",
    "\n",
    "GS0['body'] = body\n",
    "\n",
    "GS = GS0[:]\n",
    "GS.dtypes\n",
    "\n",
    "\n",
    "data = GS[:]\n",
    "\n",
    "\n",
    "\n",
    "GS_P = pd.read_csv(\"C:/Users/simeon/Documents/GS_P.csv\")\n",
    "\n",
    "\n",
    "GS_P = GS_P.rename(columns={'Close': 'CLOSE'})\n",
    "\n",
    "\n",
    "NKE = pd.read_json(\"C:/Users/simeon/Documents/NKE.txt\")\n",
    "NKE_P = pd.read_csv(\"C:/Users/simeon/Documents/NKE.csv\")\n",
    "MCD = pd.read_json(\"C:/Users/simeon/Documents/MCD.txt\")\n",
    "MCD_P = pd.read_csv(\"C:/Users/simeon/Documents/MCD.csv\")\n",
    "DIS = pd.read_json(\"C:/Users/simeon/Documents/DIS.txt\")\n",
    "DIS_P = pd.read_csv(\"C:/Users/simeon/Documents/DIS.csv\")\n",
    "WMT = pd.read_json(\"C:/Users/simeon/Documents/WMT.txt\")\n",
    "WMT_P = pd.read_csv(\"C:/Users/simeon/Documents/WMT.csv\")\n",
    "VZ = pd.read_json(\"C:/Users/simeon/Documents/VZ.txt\")\n",
    "VZ_P = pd.read_csv(\"C:/Users/simeon/Documents/VZ.csv\")\n",
    "V = pd.read_json(\"C:/Users/simeon/Documents/V.txt\")\n",
    "V_P = pd.read_csv(\"C:/Users/simeon/Documents/V.csv\")\n",
    "UTX = pd.read_json(\"C:/Users/simeon/Documents/UTX.txt\")\n",
    "UTX_P = pd.read_csv(\"C:/Users/simeon/Documents/UTX.csv\")\n",
    "UNH = pd.read_json(\"C:/Users/simeon/Documents/UNH.txt\")\n",
    "UNH_P = pd.read_csv(\"C:/Users/simeon/Documents/UNH.csv\")\n",
    "TRV = pd.read_json(\"C:/Users/simeon/Documents/TRV.txt\")\n",
    "TRV_P = pd.read_csv(\"C:/Users/simeon/Documents/TRV.csv\")\n",
    "CSCO = pd.read_json(\"C:/Users/simeon/Documents/CSCO.txt\")\n",
    "CSCO_P = pd.read_csv(\"C:/Users/simeon/Documents/CSCO.csv\")\n",
    "CAT = pd.read_json(\"C:/Users/simeon/Documents/CAT.txt\")\n",
    "CAT_P = pd.read_csv(\"C:/Users/simeon/Documents/CAT.csv\")\n",
    "PG = pd.read_json(\"C:/Users/simeon/Documents/PG.txt\")\n",
    "PG_P = pd.read_csv(\"C:/Users/simeon/Documents/PG.csv\")\n",
    "PFE= pd.read_json(\"C:/Users/simeon/Documents/PFE.txt\")\n",
    "PFE_P = pd.read_csv(\"C:/Users/simeon/Documents/PFE.csv\")\n",
    "MSFT = pd.read_json(\"C:/Users/simeon/Documents/MSFT.txt\")\n",
    "MSFT_P = pd.read_csv(\"C:/Users/simeon/Documents/MSFT.csv\")\n",
    "MRK = pd.read_json(\"C:/Users/simeon/Documents/MRK.txt\")\n",
    "MRK_P = pd.read_csv(\"C:/Users/simeon/Documents/MRK.csv\")\n",
    "MMM= pd.read_json(\"C:/Users/simeon/Documents/MMM.txt\")\n",
    "MMM_P = pd.read_csv(\"C:/Users/simeon/Documents/MMM.csv\")\n",
    "JPM = pd.read_json(\"C:/Users/simeon/Documents/JPM.txt\")\n",
    "JPM_P = pd.read_csv(\"C:/Users/simeon/Documents/JPM.csv\")\n",
    "JNJ = pd.read_json(\"C:/Users/simeon/Documents/JNJ.txt\")\n",
    "JNJ_P = pd.read_csv(\"C:/Users/simeon/Documents/JNJ.csv\")\n",
    "INTC = pd.read_json(\"C:/Users/simeon/Documents/INTC.txt\")\n",
    "INTC_P = pd.read_csv(\"C:/Users/simeon/Documents/INTC.csv\")\n",
    "IBM = pd.read_json(\"C:/Users/simeon/Documents/IBM.txt\")\n",
    "IBM_P = pd.read_csv(\"C:/Users/simeon/Documents/IBM.csv\")\n",
    "HD = pd.read_json(\"C:/Users/simeon/Documents/HD.txt\")\n",
    "HD_P = pd.read_csv(\"C:/Users/simeon/Documents/HD.csv\")\n",
    "GE = pd.read_json(\"C:/Users/simeon/Documents/GE.txt\")\n",
    "GE_P = pd.read_csv(\"C:/Users/simeon/Documents/GE.csv\")\n",
    "XOM = pd.read_json(\"C:/Users/simeon/Documents/XOM.txt\")\n",
    "XOM_P = pd.read_csv(\"C:/Users/simeon/Documents/XOM.csv\")\n",
    "DD = pd.read_json(\"C:/Users/simeon/Documents/DD.txt\")\n",
    "DD_P = pd.read_csv(\"C:/Users/simeon/Documents/DD.csv\")\n",
    "KO = pd.read_json(\"C:/Users/simeon/Documents/KO.txt\")\n",
    "KO_P = pd.read_csv(\"C:/Users/simeon/Documents/KO.csv\")\n",
    "CVX = pd.read_json(\"C:/Users/simeon/Documents/CVX.txt\")\n",
    "CVX_P = pd.read_csv(\"C:/Users/simeon/Documents/CVX.csv\")\n",
    "BA = pd.read_json(\"C:/Users/simeon/Documents/BA.txt\")\n",
    "BA_P = pd.read_csv(\"C:/Users/simeon/Documents/BA.csv\")\n",
    "AXP = pd.read_json(\"C:/Users/simeon/Documents/AXP.txt\")\n",
    "AXP_P = pd.read_csv(\"C:/Users/simeon/Documents/AXP.csv\")\n",
    "FB = pd.read_json(\"C:/Users/simeon/Documents/FB.txt\")\n",
    "FB_P = pd.read_csv(\"C:/Users/simeon/Documents/FB.csv\")\n",
    "QQQ = pd.read_json(\"C:/Users/simeon/Documents/QQQ.txt\")\n",
    "QQQ_P = pd.read_csv(\"C:/Users/simeon/Documents/QQQ.csv\")\n",
    "SPY = pd.read_json(\"C:/Users/simeon/Documents/SPY.txt\")\n",
    "SPY_P = pd.read_csv(\"C:/Users/simeon/Documents/SPY.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "shebang(NKE, NKE_P, \"$NKE\")  \n",
    "NKE = F[:]     \n",
    "shebang(MCD, MCD_P, \"$MCD\")  \n",
    "MCD = F[:]  \n",
    "shebang(DIS, DIS_P, \"$DIS\")  \n",
    "DIS = F[:]  \n",
    "shebang(WMT, WMT_P, \"WMT\")  \n",
    "WMT = F[:]   \n",
    "shebang(VZ, VZ_P, \"$VZ\")  \n",
    "VZ = F[:] \n",
    "shebang(V, V_P, \"$V\")  \n",
    "V = F[:] \n",
    "shebang(UTX, UTX_P, \"$UTX\")  \n",
    "UTX = F[:] \n",
    "shebang(UNH, UNH_P, \"$UNH\")  \n",
    "UNH = F[:] \n",
    "shebang(TRV, TRV_P, \"$TRV\")  \n",
    "TRV = F[:] \n",
    "shebang(CSCO, CSCO_P, \"$CSCO\")  \n",
    "CSCO = F[:] \n",
    "shebang(CAT, CAT_P, \"$CAT\")  \n",
    "CAT = F[:]\n",
    "shebang(PG, PG_P, \"$PG\")  \n",
    "PG = F[:]\n",
    "shebang(PFE, PFE_P, \"$PFE\")  \n",
    "PFE = F[:]\n",
    "shebang(MSFT, MSFT_P, \"$MSFT\")  \n",
    "MSFT = F[:]\n",
    "shebang(MRK, MRK_P, \"$MRK\")  \n",
    "MRK = F[:]\n",
    "shebang(MMM, MMM_P, \"$MMM\")  \n",
    "MMM = F[:]\n",
    "shebang(JPM, JPM_P, \"$JPM\")  \n",
    "JPM = F[:]\n",
    "shebang(JNJ, JNJ_P, \"$JNJ\")  \n",
    "JNJ = F[:]\n",
    "shebang(INTC, INTC_P, \"$INTC\")  \n",
    "INTC = F[:]\n",
    "shebang(IBM, IBM_P, \"$IBM\")  \n",
    "IBM = F[:]\n",
    "shebang(HD, HD_P, \"$HD\")  \n",
    "HD = F[:]\n",
    "shebang(GS, GS_P, \"$GS\")  \n",
    "GS = F[:]\n",
    "shebang(GE, GE_P, \"$GE\")  \n",
    "GE = F[:]\n",
    "shebang(XOM, XOM_P, \"$XOM\")  \n",
    "XOM = F[:]\n",
    "shebang(DD, DD_P, \"$DD\")  \n",
    "DD = F[:]\n",
    "shebang(KO, KO_P, \"$KO\")  \n",
    "KO = F[:]\n",
    "shebang(CVX, CVX_P, \"$CVX\")  \n",
    "CVX = F[:]\n",
    "shebang(BA, BA_P, \"$BA\")  \n",
    "BA = F[:]\n",
    "shebang(AXP, AXP_P, \"$AXP\")  \n",
    "AXP = F[:]\n",
    "shebang(FB, FB_P, \"$FB\")  \n",
    "FB = F[:]\n",
    "shebang(QQQ, QQQ_P, \"$QQQ\")  \n",
    "QQQ = F[:]\n",
    "shebang(SPY, SPY_P, \"$SPY\")  \n",
    "SPY = F[:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GS0.to_csv('C:/Users/simeon/Documents/GS0_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "def W_M(data):\n",
    "    df_new = data[\"tweets_no_stop\"][data[\"tweets_no_stop\"].notnull()]\n",
    "    all_words = [] \n",
    "    for i in range(len(df_new)):\n",
    "        print(i)\n",
    "        a = df_new[i].rsplit()\n",
    "        all_words.extend(a) \n",
    "    global add\n",
    "    add = all_words\n",
    "        \n",
    "    \n",
    "full_list = []   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W_M(NKE) \n",
    "full_list.extend(add)  \n",
    "\n",
    "    \n",
    "W_M(MCD) \n",
    "full_list.extend(add)    \n",
    "    \n",
    "W_M(DIS) \n",
    "full_list.extend(add) \n",
    "W_M(WMT) \n",
    "full_list.extend(add)\n",
    "W_M(VZ) \n",
    "full_list.extend(add)\n",
    "W_M(V) \n",
    "full_list.extend(add)\n",
    "W_M(UTX) \n",
    "full_list.extend(add)\n",
    "W_M(UNH) \n",
    "full_list.extend(add)\n",
    "W_M(TRV) \n",
    "full_list.extend(add)\n",
    "W_M(CSCO) \n",
    "full_list.extend(add)\n",
    "W_M(CAT) \n",
    "full_list.extend(add)\n",
    "W_M(PG) \n",
    "full_list.extend(add)\n",
    "W_M(PFE) \n",
    "full_list.extend(add)\n",
    "W_M(MSFT) \n",
    "full_list.extend(add)\n",
    "W_M(MRK) \n",
    "full_list.extend(add)\n",
    "W_M(MMM) \n",
    "full_list.extend(add)\n",
    "W_M(JPM) \n",
    "full_list.extend(add)\n",
    "W_M(JNJ) \n",
    "full_list.extend(add)\n",
    "W_M(INTC) \n",
    "full_list.extend(add)\n",
    "W_M(IBM) \n",
    "full_list.extend(add)\n",
    "W_M(HD) \n",
    "full_list.extend(add)\n",
    "W_M(GS) \n",
    "full_list.extend(add)\n",
    "W_M(GE) \n",
    "full_list.extend(add)\n",
    "W_M(XOM) \n",
    "full_list.extend(add)\n",
    "W_M(DD) \n",
    "full_list.extend(add)\n",
    "W_M(KO) \n",
    "full_list.extend(add)\n",
    "W_M(CVX) \n",
    "full_list.extend(add)\n",
    "W_M(BA) \n",
    "full_list.extend(add)\n",
    "W_M(AXP) \n",
    "full_list.extend(add)\n",
    "W_M(FB) \n",
    "full_list.extend(add)\n",
    "W_M(QQQ) \n",
    "full_list.extend(add)\n",
    "W_M(SPY) \n",
    "full_list.extend(add)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "th = Counter(full_list) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key, count in dropwhile(lambda key_count: key_count[1] >= 25, th.most_common()):\n",
    "    del th[key]\n",
    "    \n",
    "    \n",
    "everything = list(set(full_list))   \n",
    "    \n",
    "keep = list(th.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l2s = set(keep)\n",
    "\n",
    "rid = [x for x in everything if x not in l2s]\n",
    "\n",
    "mylist = list(set(rid))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def uncommon_word_rem(Input_data):\n",
    "    \n",
    "    Input_data = pd.DataFrame(Input_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(mylist))\n",
    "    Input_data['bodyn'] = Input_data[\"tweets_no_stop\"].astype(str).str.replace(pat, '')\n",
    "    Input_data[\"tweets_no_stop\"] = Input_data['bodyn'].str.replace(r'\\s+', ' ')\n",
    "    global new\n",
    "    new = Input_data[:]\n",
    "\n",
    "\n",
    "\n",
    "uncommon_word_rem(NKE)\n",
    "NKE = new\n",
    "uncommon_word_rem(MCD)\n",
    "MCD = new\n",
    "uncommon_word_rem(DIS)\n",
    "DIS = new\n",
    "uncommon_word_rem(WMT)\n",
    "WMT = new\n",
    "uncommon_word_rem(VZ)\n",
    "VZ = new\n",
    "uncommon_word_rem(V)\n",
    "V = new\n",
    "uncommon_word_rem(UTX)\n",
    "UTX = new\n",
    "uncommon_word_rem(UNH)\n",
    "UNH = new\n",
    "uncommon_word_rem(TRV)\n",
    "TRV = new\n",
    "uncommon_word_rem(CSCO)\n",
    "CSCO = new\n",
    "uncommon_word_rem(CAT)\n",
    "CAT = new\n",
    "uncommon_word_rem(PG)\n",
    "PG = new\n",
    "uncommon_word_rem(PFE)\n",
    "PFE = new\n",
    "uncommon_word_rem(MSFT)\n",
    "MSFT = new\n",
    "uncommon_word_rem(MRK)\n",
    "MRK = new\n",
    "uncommon_word_rem(MMM)\n",
    "MMM = new\n",
    "uncommon_word_rem(JPM)\n",
    "JPM = new\n",
    "uncommon_word_rem(JNJ)\n",
    "JNJ = new\n",
    "uncommon_word_rem(INTC)\n",
    "INTC = new\n",
    "uncommon_word_rem(IBM)\n",
    "IBM = new\n",
    "uncommon_word_rem(HD)\n",
    "HD = new\n",
    "\n",
    "uncommon_word_rem(GE)\n",
    "GE = new\n",
    "uncommon_word_rem(XOM)\n",
    "XOM = new\n",
    "uncommon_word_rem(DD)\n",
    "DD = new\n",
    "uncommon_word_rem(KO)\n",
    "KO = new\n",
    "uncommon_word_rem(CVX)\n",
    "CVX = new\n",
    "uncommon_word_rem(BA)\n",
    "BA= new\n",
    "uncommon_word_rem(AXP)\n",
    "AXP = new\n",
    "uncommon_word_rem(FB)\n",
    "FB = new\n",
    "uncommon_word_rem(QQQ)\n",
    "QQQ = new\n",
    "uncommon_word_rem(SPY)\n",
    "SPY = new\n",
    "\n",
    "\n",
    "\n",
    "uncommon_word_rem(GS)\n",
    "GS = new[:]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000, lowercase=True, analyzer='word',\n",
    "stop_words= 'english',ngram_range=(1,1))\n",
    "\n",
    "\n",
    "\n",
    "train_vect = tfidf.fit_transform(GS[\"tweets_no_stop\"]\n",
    ")\n",
    "dense = train_vect.todense()\n",
    "dense = pd.DataFrame(dense)\n",
    "GS = pd.concat([GS, dense.set_index(GS.index)], axis=1)\n",
    "\n",
    "\n",
    "GS['1buy'] = np.where(GS['1_day_return']>=0, 1, 0)\n",
    "GS['3buy'] = np.where(GS['3_day_return']>=0, 1, 0)\n",
    "GS['5buy'] = np.where(GS['5_day_return']>=0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x =GS[np.isfinite(GS['tot_tweet'])]\n",
    "\n",
    "y_1=pd.DataFrame(x['1buy'])\n",
    "y_3=pd.DataFrame(x['3buy'])\n",
    "y_5=pd.DataFrame(x['5buy'])\n",
    "\n",
    "x= x.drop([\"Adj Close\",\"mv10\",\"1buy\",\"3buy\",\"5buy\",\"Open\",\"High\",\"Low\",\"Volume\",\"polarity\",'1_day_return',\"CLOSE\",\"3_day_return\",\"5_day_return\",\"tweets\",\"roll_avg\",\"percent_change\",\"tweets_no_stop\",\"bodyn\"], axis=1)          \n",
    "\n",
    "#GS.to_csv(r'C:/Users/simeon/Documents/GS_test.csv')\n",
    "x.dtypes\n",
    "\n",
    "import pandas as pd\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from numpy import *\n",
    "from numpy import isnan\n",
    "\n",
    "#y_1.to_csv(r'C:/Users/simeon/Documents/y_1_test.csv')\n",
    "#X_1.to_csv(r'C:/Users/simeon/Documents/x_1_test.csv')\n",
    "\n",
    "where_are_NaNs = isnan(x)\n",
    "x[where_are_NaNs] = 0\n",
    "\n",
    "#1,000 features for 1 days returns\n",
    "X_1=SelectKBest(chi2,k=1000).fit_transform(x,y_1)\n",
    "X_1=pd.DataFrame(X_1)\n",
    "\n",
    "#1,000 features for 3 days returns\n",
    "X_3=SelectKBest(chi2,k=1000).fit_transform(x,y_3)\n",
    "X_3=pd.DataFrame(X_3)\n",
    "\n",
    "\n",
    "#1,000 features for 5 days returns\n",
    "\n",
    "X_5=SelectKBest(chi2,k=1000).fit_transform(x,y_5)\n",
    "X_5=pd.DataFrame(X_5)\n",
    "\n",
    "# split \n",
    "X_train_1, X_test_1, y_train_1, y_test_1 =X_1[:756], X_1[756:], y_1[:756], y_1[756:]\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 =X_3[:588], X_3[589:], y_3[:588], y_3[589:]\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 =X_5[:588], X_5[589:], y_5[:588], y_5[589:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1 day ahead \n",
    "\n",
    "X_train=X_train_1\n",
    "y_train=y_train_1\n",
    "\n",
    "X_test=X_test_1\n",
    "y_test=y_test_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "cm=confusion_matrix(y_test, y_pred)\n",
    "print( \"Accurancy=\", clf.score(X_test, y_test) *100, \"%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
